None of our solutions to this problem required extra set up. Everything can be run directly in terminal using this command "python [code file name] [input file location as a string] [desired output file location as a string]." The file locations are not absolute. Here is an example: python solver.py "phase2_inputs/inputs20/input20_0.in" "outputs/output20_0.out"

The input folders should be placed in the same folder of the code files (ex. phase2_inputs and solver.py should be on the same level).

The first algorithm we tried to use to solve this problem was decision tree pruning. We did this by adding wizards to the ordering until adding any remaining wizard breaking the constraints. This was recorded as a mapping from each wizard to the wizards that had to come before him. For example, if a constraint (a b c) existed and a was added to the ordering, then c would be added to b's required predecessors list. If no more wizards can be added to the current ordering and more wizards still remain, then we backtrack to try adding a different wizard. 

We improved on this algorithm by intelligently ordering the wizards to be iterated upon, first iterating over decreasing numbers of appearances as the third name in constraints (so more constraints were immediately satisfied) and then over increasing numbers of appearances as the one of the first two names in constraints (so less restrictions were immediately added as required predecessors). We used this algorithm on all of the 20s and 35s and finished everything in under 2 minutes. 

Since the decision tree pruning algorithm was unable to solve any of the 50s, even after being run for hours, we started on a new algorithm. We tried to start with a suboptimal solution (made by placing the names that appear least as the first two names on the outside and the ones that appear the most on the inside) and then swap each name to the position in which the most number of constraints are satisfied. Each name must be tried in every position in the ordering, so each iteration of this took O(n^2) time. We had high hopes since it was much better than our previous O(n!), but unfortunately this algorithm never did a bad swap to allow for better swap latter, so it was only able to find the local maximum of satisfied constraints, not the optimal. We realized that we had to be willing to make bad swaps, so we did some intese Googling and settled on simulated annealing. (The remnants of this attempt are in the loopOnce function -- it is still useful for getting closer to the goal quickly, even if it can't reach the optimal.)

Our simulated annealing algorithm randomly selected 2 names, and, if the swap increased the number of the constraaints satisfied it did the swap immediately. If the swap was not beneficial we had a decreasing probabilistic function to determine whether or not we accepted the swap. The probability that the suboptimal swap happened was e^[(new number of constraints satsfied - original number of constraints satisfied) / T]. T is initialized at 1 and decreases after every swap, resulting in a lower chance that a suboptimal swap happens the longer the algorithm has been running. We solved 6 on the 50s using this method, but some of the them plateaued out and remained there for over an hour. We realized that (1) we had to choose constraints that keep it from plateauing and (2) we couldn't let the algorithm waste time rejecting most of the swaps. 

We tried to select our constraints more intelligently to force the unsatisfied constraints. In the end we did this by randomly selecting an unsatisfied constraint and then doing one of the swaps that satisfied it (if we picked constraint (a b c) which was unsatisfied, that meant c is currently between a and b and the swaps (c a) or (c b) would break it). This also made it possible for us to make a swap every iteration while still selecting intelligently. 