None of our solutions to this problem required extra set up. Everything can be run directly in terminal using this command "python [code file name] [input file location as a string] [desired output file location as a string]." The file locations are not absolute. Here is an example: python solver.py "phase2_inputs/inputs20/input20_0.in" "outputs/output20_0.out"

The input folders should be placed in the same folder of the code files (ex. phase2_inputs and solver.py should be on the same level).

The first algorithm we tried to use to solve this problem was decision tree pruning. We did this by adding wizards to the ordering until adding any remaining wizard breaking the constraints. This was recorded as a mapping from each wizard to the wizards that had to come before him. For example, if a constraint (a b c) existed and a was added to the ordering, then c would be added to b's required predecessors list. If no more wizards can be added to the current ordering and more wizards still remain, then we backtrack to try adding a different wizard.

We improved on this algorithm by intelligently ordering the wizards to be iterated upon, first iterating over decreasing numbers of appearances as the third name in constraints (so more constraints were immediately satisfied) and then over increasing numbers of appearances as the one of the first two names in constraints (so less restrictions were immediately added as required predecessors). We used this algorithm on all of the 20s and 35s and finished everything in under 2 minutes.

Since the decision tree pruning algorithm was unable to solve any of the 50s, even after being run for hours, we started on a new algorithm. We tried to start with a suboptimal solution (made by placing the names that appear least as the first two names on the outside and the ones that appear the most on the inside) and then swap each name to the position in which the most number of constraints are satisfied. Each name must be tried in every position in the ordering, so each iteration of this took O(n^2) time. We had high hopes since it was much better than our previous O(n!), but unfortunately this algorithm never did a bad swap to allow for better swap latter, so it was only able to find the local maximum of satisfied constraints, not the optimal. We realized that we had to be willing to make bad swaps, so we did some intense Googling and settled on simulated annealing. (The remnants of this attempt are in the swapAll function -- it is still useful for getting closer to the goal quickly, even if it can't reach the optimal.)

Our simulated annealing algorithm randomly selected 2 names, and, if the swap increased the number of the constraints satisfied it did the swap immediately. If the swap was not beneficial we had a decreasing probabilistic function to determine whether or not we accepted the swap. The probability that the suboptimal swap happened was e^[(new number of constraints satisfied - original number of constraints satisfied) / T]. T is initialized at 1 and decreases after every swap, resulting in a lower chance that a suboptimal swap happens the longer the algorithm has been running. We solved 6 on the 50s using this method, but some of the them plateaued out and remained there for over an hour. We realized that (1) we had to choose constraints that keep it from plateauing and (2) we couldn't let the algorithm waste time rejecting most of the swaps.

We tried to select our constraints more intelligently to force the unsatisfied constraints. In the end we did this by randomly selecting an unsatisfied constraint and then doing one of the swaps that satisfied it (if we picked constraint (a b c) which was unsatisfied, that would mean c is currently between a and b and the swaps (c a) or (c b) would satisfy it). This also made it possible for us to make a swap every iteration while still selecting intelligently.

Our final strategy was influenced by simulated annealing and another strategy called Epsilon decreasing. Basically we would look at two potential swaps and identify which among them satisfied more constraints (optimal) and which one satisfied fewer (suboptimal). Then we probabilistically chose between them such that the probability of choosing the suboptimal was always less than optimal but still nonzero. The point of this was that we could implement a rough hill-climbing algorithm, but it had an escape for local maxima (or minima, depending on your POV), so you don't get stuck. The advantage over SA was that there is no option for stasis - you're always moving in one direction or another. This is good since this algorithm is essentially a smart random algorithm, so the more points you get to explore, the more likely it is that you'll find an optimal ordering.

We used the tree pruning algorithm to solve all the 20s and 35s, then we used SA for 6 of the 50s but discovered that our final algorithm was far superior. So, to run our code and get the solutions in the same way we did, use the pruning for 20s and 35s, which should mostly finish within 1 minute, with a few taking up to 15 minutes. The SA 50 wizard files all finished within ten minutes, then the Epsilon decreasing strategy finished everything up to 120 within 2 minutes and the rest of them that we finished were done within 15 minutes. 
